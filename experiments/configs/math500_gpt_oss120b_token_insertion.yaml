# Configuration for token insertion experiment
# This experiment:
# 1. Loads previously sampled responses with thinking traces
# 2. Inserts random tokens into the thinking trace (equal to N% of original trace length)
# 3. Closes the thinking tag and prefills it in the prompt
# 4. Uses OpenRouter completions API to generate only the final answer after the thinking tag

# Model configuration
model:
  name: "openai/gpt-oss-120b"
  temperature: 0.6
  max_tokens: 32768
  top_p: 0.95
  reasoning: true
  logprobs: false
  top_logprobs: 5

# OpenRouter configuration
openrouter:
  providers: ["DeepInfra"]

# Experiment configuration
experiment:
  # Input: previously sampled responses
  input_responses_dir: "experiments/results_final/gpt-oss-120b/math500/samples"

  # Which prompts to process
  start_index: 0
  end_index: null  # Process all prompts
  # Note: ALL responses per prompt will be processed

  # Token insertion configuration
  # percentage_to_insert: 0.5 means insert tokens equal to 50% of original trace length
  # (e.g., if original trace has 100 tokens, insert 50 random tokens)
  percentage_to_insert: 0.25  # Percentage of original trace length to insert (0.5 = 50%)
  # n_tokens_to_insert: 50  # Alternative: Number of random tokens to insert
  cut_start_percentage: 0.0  # Start percentage for cutting (0.0 = from beginning)
  cut_end_percentage: 0.5  # End percentage for cutting (0.5 = up to 50%, keeps tokens from start% to end%)
  # Thinking tag mode
  close_thinking_tag: false  # TRUE = close tag, model generates only answer after thinking tag

  # Save original response in output JSON
  save_original_response: false  # If true, saves the original response data in the output JSON file

  # Random seed for reproducibility
  seed: 42

  # Tokenizer configuration (required for proper tokenization)
  tokenizer_name: "openai/gpt-oss-120b"  # Tokenizer name from transformers library

  # Batch size for parallel processing
  batch_size: 10

  # Output configuration
  output_dir: "experiments/results_final/gpt-oss-120b/math500/token_insertion_open/cut_0.5/insert_0.25"

prompt_formatting:
  prefix_user_tokens: "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\nKnowledge cutoff: 2024-06\n\nReasoning: high\n\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|>\n<|start|>user<|message|>"
  postfix_user_tokens: "<|end|>\n"  # Optional: e.g., "<|im_end|>\n" for some models
  prefix_assistant_tokens: "<|start|>assistant"
  open_thinking_token: "<|channel|>analysis<|message|>"
  close_thinking_token: "<|end|>"

# Optional: Prompt template configuration
# If specified, reformats the prompt using the template before building completion prompts
prompt:
  template: "math500_tampering"  # Template name from PROMPT_TEMPLATES in src/prompt_formatter.py
  field_name: "problem"  # Field name to extract from dataset
  # custom_template: null  # Optional: custom template string with {} placeholder (overrides template)
