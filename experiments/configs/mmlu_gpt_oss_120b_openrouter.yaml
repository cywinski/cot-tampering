# Configuration for MATH-500 dataset sampling with DeepSeek-R1 via OpenRouter
# Note: OpenRouter returns reasoning traces in separate fields from the final answer,
# but the client automatically combines them into a single field for compatibility with Nebius.

# Model configuration
model:
  provider: "openrouter"  # Provider: "nebius" or "openrouter"
  name: "openai/gpt-oss-120b"  # DeepSeek reasoning model
  temperature: 0.6  # Sampling temperature (0.0-2.0, higher = more random)
  max_tokens: 16384  # Maximum tokens to generate per response (reasoning models benefit from longer context for reasoning)
  top_p: 0.95  # Nucleus sampling threshold
  top_k: null  # Top-k sampling parameter

  # OpenRouter-specific options
  reasoning: true  # Enable reasoning mode - saves both reasoning trace and final answer
  logprobs: false  # Return log probabilities
  top_logprobs: 5  # Number of top logprobs per token

openrouter:
  providers: ["DeepInfra"]

# Sampling configuration
sampling:
  n_responses: 5  # Number of responses to sample per problem
  batch_size: 20  # Number of responses to sample per batch
  max_retries: 3  # Maximum retries for failed API calls
  timeout: null  # Timeout in seconds (null = no timeout, important for reasoning models that may take very long)

# Dataset configuration
dataset:
  name: "mmlu"
  params:
    subject: "marketing"
  random_sample: null  # Randomly sample N problems from dataset (null to use all)
  random_seed: 42  # Random seed for reproducibility

# Prompt formatting
prompt:
  template: "mmlu_multiple_choice"  # Template name from PROMPT_TEMPLATES in src/prompt_formatter.py
  field_name: "question"  # Field name to extract from dataset
  # custom_template: null  # Optional: custom template string with {} placeholder (overrides template)

prompt_formatting:
  prefix_user_tokens: "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\nKnowledge cutoff: 2024-06\n\nReasoning: high\n\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|>\n<|start|>user<|message|>"
  postfix_user_tokens: "<|end|>\n"  # Optional: e.g., "<|im_end|>\n" for some models
  prefix_assistant_tokens: "<|start|>assistant"
  thinking_token: ""  # Tag used in source responses (e.g., "think", "redacted_reasoning")
# Output configuration
output:
  save_dir: "experiments/results_final/gpt-oss-120b/mmlu/marketing/samples"  # Base directory for results (subdirectory per dataset will be created)
  # Each prompt's responses saved to individual JSON files: prompt_0000.json, prompt_0001.json, etc.
