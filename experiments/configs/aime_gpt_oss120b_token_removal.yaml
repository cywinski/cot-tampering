# Configuration for token removal experiment (CLOSED THINKING MODE)
# This experiment:
# 1. Loads previously sampled responses with thinking traces
# 2. Removes N% of random tokens from the thinking trace
# 3. Closes the </think> tag and prefills it in the prompt
# 4. Uses OpenRouter completions API to generate only the final answer after </think>

# Model configuration
model:
  name: "openai/gpt-oss-120b"
  temperature: 0.6
  max_tokens: 32768
  top_p: 0.95
  reasoning: true
  logprobs: false
  top_logprobs: 5

# OpenRouter configuration
openrouter:
  providers: ["DeepInfra"]

# Experiment configuration
experiment:
  # Input: previously sampled responses
  input_responses_dir: "experiments/results_final/gpt-oss-120b/aime2024/samples"

  # Which prompts to process
  start_index: 0
  end_index: 1  # Process first 10 for testing
  # Note: ALL responses per prompt will be processed

  # Token removal configuration
  # Can specify cut_at_percentage OR (cut_start_percentage + cut_end_percentage), optionally combined with removal
  # If both cut and removal are specified: first cut the trace, then remove tokens from the remaining portion
  # cut_at_percentage: 0.5  # Cut trace at percentage - keep first X% of tokens from beginning (0.5 = 50% = keep first half)
  cut_start_percentage: 0.0  # Start percentage for cutting (0.0 = from beginning)
  cut_end_percentage: 0.5  # End percentage for cutting (0.5 = up to 50%, keeps tokens from start% to end%)
  percentage_to_remove: 0.5  # Percentage of random tokens to remove from the cut trace (0.5 = 50%)
  # n_tokens_to_remove: 50  # Alternative: Number of random tokens to remove from the cut trace

  # Thinking tag mode
  close_thinking_tag: false  # TRUE = close tag, model generates only answer after </think>

  # Save original response in output JSON
  save_original_response: false  # If true, saves the original response data in the output JSON file

  # Random seed for reproducibility
  seed: 42

  # Tokenizer configuration (required for proper tokenization)
  tokenizer_name: "openai/gpt-oss-120b"  # Tokenizer name from transformers library

  # Output configuration
  output_dir: "experiments/results_final/gpt-oss-120b/aime2024/token_removal_open/cut_0.5/remove_0.5"

prompt_formatting:
  prefix_user_tokens: "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\nKnowledge cutoff: 2024-06\n\nReasoning: high\n\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|>\n<|start|>user<|message|>"
  postfix_user_tokens: "<|end|>\n"  # Optional: e.g., "<|im_end|>\n" for some models
  prefix_assistant_tokens: "<|start|>assistant"
  open_thinking_token: "<|channel|>analysis<|message|>"
  close_thinking_token: "<|end|>"

# Optional: Prompt template configuration
# If specified, reformats the prompt using the template before building completion prompts
prompt:
  template: "math500_tampering"  # Template name from PROMPT_TEMPLATES in src/prompt_formatter.py
  field_name: "problem"  # Field name to extract from dataset
  # custom_template: null  # Optional: custom template string with {} placeholder (overrides template)
